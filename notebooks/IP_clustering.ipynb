{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Common Imports\n",
    "import numpy as np\n",
    "#Pandas for creating dataframes\n",
    "import pandas as pd\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "#K-means clustering algo\n",
    "from sklearn.cluster import KMeans\n",
    "#OS moduled for file oprations\n",
    "import os\n",
    "#CSV module\n",
    "import csv\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
    "def PCA_component_analysis(X_std):\n",
    "    mean_vec = np.mean(X_std, axis=0)\n",
    "    cov_mat = np.cov(X_std.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "    # Create a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "    # # Sort from high to low\n",
    "    eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "    # Calculation of Explained Variance from the eigenvalues\n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "    cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "    # PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(9), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
    "    plt.step(range(9), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def draw_clusters(X, X_clusters, centroids, kmeans):\n",
    "    #Use PCA component analysis for visuals\n",
    "    if X.shape[1] > 2:\n",
    "        reduced_X = PCA(n_components=2).fit_transform(X)\n",
    "    else:\n",
    "        reduced_X = X\n",
    "   \n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .01     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_X[:, 0].min() - 1, reduced_X[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_X[:, 1].min() - 1, reduced_X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')   \n",
    "    #Plot the data points (PCA reduced components)\n",
    "    plt.plot(reduced_X[:,0],reduced_X[:,1],  'k.', markersize=3) \n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=169, linewidths=3, color='w', zorder=10)\n",
    "    plt.title('K-means clustering with (PCA-reduced data), Centroids are marked with white cross')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "#Calculate distance of data point from the its cluster center\n",
    "def k_mean_dist(data, clusters, cluster_centers):\n",
    "    distances = []\n",
    "    for i, d in enumerate(data):\n",
    "        center = cluster_centers[clusters[i]]\n",
    "        distance = euclidean(d,center)\n",
    "        #distance = np.linalg.norm(d - center)\n",
    "        distances.append(distance)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#Find optimal number of clusters for k-means clustering using elbow method.\n",
    "def elbow_method(X_trans):\n",
    "    elbow_count = 0\n",
    "    range_val = 10\n",
    "    Nc = range(1, range_val)\n",
    "    kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "    score = [kmeans[i].fit(X_trans).score(X_trans) for i in range(len(kmeans))]\n",
    "    total_diff = abs(score[0] - score[len(score) -1])\n",
    "    for i in range(range_val - 2):\n",
    "        percent_diff = abs(score[i] - score[i+1])/total_diff\n",
    "        if percent_diff < 0.01:\n",
    "            elbow_count = i\n",
    "            break\n",
    "    plt.plot(Nc,score)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Elbow Curve')\n",
    "    plt.show()\n",
    "    return elbow_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "#Merge sample files to create bigger sameple\n",
    "def merge_sample_files(sample_folder, file_count):\n",
    "    file_number = 1\n",
    "    count = 0\n",
    "    filenames = sorted(glob.glob(os.path.join(sample_folder,'*')),  key=os.path.getmtime)\n",
    "    for filename in filenames:\n",
    "        if count == 0:\n",
    "            df = pd.read_csv(filename, index_col=0)\n",
    "            count += 1\n",
    "        else:\n",
    "            temp_df = pd.read_csv(filename, index_col=0)\n",
    "            df = df.append(temp_df)\n",
    "            count += 1\n",
    "        if count == file_count:\n",
    "            df.to_csv(os.path.join(sample_folder,'m'+str(file_number)))\n",
    "            df = df.drop(df.index, inplace=True)\n",
    "            count = 0\n",
    "            file_number +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_feature_vector_dict(cluster_folder):\n",
    "    cluster_dict = dict()\n",
    "    filenames = sorted(glob.glob(os.path.join(cluster_folder,'*')),  key=os.path.getmtime)\n",
    "    first = True\n",
    "    for filename in filenames:\n",
    "        if first:\n",
    "            df = pd.read_csv(filename, index_col=0)\n",
    "            first = False\n",
    "        else:\n",
    "            temp_df = pd.read_csv(filename, index_col=0)\n",
    "            df = df.append(temp_df) \n",
    "    df = df.reset_index().set_index(['cluster','ip'])\n",
    "    clusters = df.index.get_level_values(0).unique()\n",
    "    for c in clusters:\n",
    "        cluster_dict[c] = df.loc[c].iloc[:,:-1].values\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_outlier_detecton(X_train, clf):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "    \n",
    "    # plot the levels lines and the points\n",
    "    print(clf.name)\n",
    "    if clf.name == \"lof\":\n",
    "        # decision_function is private for LOF\n",
    "        Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    print(Z.max(), Z.min())\n",
    "    plt.title(\"Novelty Detection\")\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "    a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "    #plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "    \n",
    "    s = 40\n",
    "    b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\n",
    "    plt.axis('tight')\n",
    "    plt.xlim((-5, 5))\n",
    "    plt.ylim((-5, 5))\n",
    "    plt.legend([a.collections[0], b1],\n",
    "           [\"learned frontier\", \"training observations\"],\n",
    "           loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SKlearn SVM\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "def one_class_svm_for_clusters(cluster_feature_dict):\n",
    "    svm_dict = dict()\n",
    "    scalar_dict = dict()\n",
    "    for key, value in cluster_feature_dict.items():\n",
    "        X_train = value\n",
    "        #Get scaler\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        scalar_dict[key] = scaler \n",
    "        #Transform Traning data\n",
    "        X_trans = scaler.transform(X_train)\n",
    "        # fit the model\n",
    "        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "        clf.fit(X_trans)\n",
    "        clf.name = 'svm'\n",
    "        plot_outlier_detecton(X_trans, clf)\n",
    "        \n",
    "        clf = LocalOutlierFactor(contamination=0.01)\n",
    "        clf.fit(X_trans)\n",
    "        clf.name= 'lof'\n",
    "        plot_outlier_detecton(X_trans, clf)\n",
    "        #Store trained SVM for each IP\n",
    "        svm_dict[key] = clf\n",
    "    return svm_dict, scalar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Base Folder Paths\n",
    "base_folder = 'converted'\n",
    "test_folder = 'test2'\n",
    "base_path = os.path.join(base_folder, test_folder)\n",
    "#Normal Sample and cluster paths\n",
    "sample_path = os.path.join(base_path,'samples')\n",
    "cluster_path = os.path.join(base_path,'ip_cluster')\n",
    "os.makedirs(cluster_path, exist_ok=True)\n",
    "#Attack Sample and Cluster paths\n",
    "# sample_path = os.path.join(base_path,'attack_samples','1')\n",
    "# cluster_path = os.path.join(base_path,'attack_ip_cluster','1')\n",
    "os.makedirs(cluster_path, exist_ok=True)\n",
    "#File to store centroids\n",
    "centroid_filename = \"centroids.csv\"\n",
    "#File to store feature list\n",
    "features_filename = \"features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge_sample_files(sample_path,3)\n",
    "d = get_cluster_feature_vector_dict(cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svm_dict, scaler_dict = one_class_svm_for_clusters(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
       "       max_iter=-1, nu=0.1, random_state=None, shrinking=True, tol=0.001,\n",
       "       verbose=False),\n",
       " 1: OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
       "       max_iter=-1, nu=0.1, random_state=None, shrinking=True, tol=0.001,\n",
       "       verbose=False),\n",
       " 2: OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
       "       max_iter=-1, nu=0.1, random_state=None, shrinking=True, tol=0.001,\n",
       "       verbose=False)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict for the give destination if it is normal or not\n",
    "X_test = [[0,120]]\n",
    "X_test_tran = scaler_dict[0].transform(X_test)\n",
    "svm_dict[0].predict(X_test_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_dataframe(sample_file, features):\n",
    "        df = pd.read_csv(sample_file, index_col=0)\n",
    "        #Filter Columns\n",
    "        df = df[['ip.dst', 'ip.proto', 'sniff_timestamp', 'sample']]\n",
    "        #Remove null destinations\n",
    "        df = df[df['ip.dst'].notnull()]\n",
    "        #Rename Columns\n",
    "        df.columns = ['ip', 'protocol', 'time_stamp', 'sample']\n",
    "        #Get count for each ip\n",
    "        df = df.groupby(['ip', 'protocol']).size().unstack().fillna(0).astype(int)\n",
    "        #Select TCP and UDP as only fetures (TCP:6, UDP:17)\n",
    "        df = df[features]\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature dataframes from the sample files\n",
    "def get_all_dataframes(sample_path, features):\n",
    "    sample_df_list = []\n",
    "    for filename in os.listdir(sample_path):\n",
    "        sample_file = os.path.join(sample_path,filename)\n",
    "        df = create_feature_dataframe(sample_file, features)\n",
    "        sample_df_list.append(df)\n",
    "    return sample_df_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kmeans_centroid(feature_df, cluster_count):\n",
    "    \"\"\" X -> feature vector \n",
    "        cluster_count -> Number of clusters to be used for k-means\n",
    "    \"\"\"\n",
    "    df_centroid = {}\n",
    "    X = feature_df.values\n",
    "    #Create scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #Transform Traning data\n",
    "    X_trans = scaler.transform(X)\n",
    "    #Data Fitting using K-means\n",
    "    kmeans = KMeans(n_clusters=cluster_count)\n",
    "    kmeans.fit(X_trans)\n",
    "    #Insert cluster center to its corrosposnding dataframe each dataframe.\n",
    "    #Dataframe 0 contain all the clusters centers associated with 0th cluster\n",
    "    first = True\n",
    "    for i in range(kmeans.cluster_centers_.shape[0]):\n",
    "        s = pd.Series(kmeans.cluster_centers_[i], index=feature_df.columns)\n",
    "        if(first):\n",
    "            df_centroid = pd.DataFrame(columns=feature_df.columns)\n",
    "            first = False\n",
    "        df_centroid = df_centroid.append(s,ignore_index=True)\n",
    "    return df_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protocol        6         17\n",
      "0        -0.166345 -0.127190\n",
      "1         7.076639  7.264017\n",
      "2         2.072327 -0.268560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpit/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "ip_dict = dict()\n",
    "sample_count = 1;\n",
    "centroid_dfs = []\n",
    "first = True\n",
    "cluster_count = 3\n",
    "features = [6,17] #(TCP:6, UDP:17)\n",
    "\n",
    "\n",
    "sample_df_list = get_all_dataframes(sample_path, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpit/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "def find_store_centroid_median(sample_frames, centroid_filename, features_filename):\n",
    "    features = sample_frames[0].columns\n",
    "    df_concat = pd.DataFrame(columns=features)\n",
    "    for df in sample_frames:\n",
    "        df_centroid = get_kmeans_centroid(df, cluster_count)\n",
    "        df_concat = df_concat.append(df_centroid)\n",
    "    #This gives list of centroid names\n",
    "    clusters = df_concat.index.unique()\n",
    "    centroids = []\n",
    "    #Find median for each centroid and store them in file\n",
    "    for c in clusters:\n",
    "        med = np.median(df_concat.loc[c], axis=0)\n",
    "        centroids.append(med) \n",
    "    np.savetxt(os.path.join(base_path, centroid_filename), np.asarray(centroids), delimiter=\",\")\n",
    "    np.savetxt(os.path.join(base_path, features_filename), np.asarray(list(features)), delimiter=\",\")\n",
    "    return np.asarray(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_centroid_features(centroid_filename, features_filename):\n",
    "    centroids = np.genfromtxt(os.path.join(base_path,centroid_filename), delimiter=',')\n",
    "    features = np.genfromtxt(os.path.join(base_path,features_filename), delimiter=',')\n",
    "    return centroids, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids, features = read_centroid_features(centroid_filename, features_filename)\n",
    "def kmeans_clustering(centroid_filename, features_filename):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuli\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Tuli\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:896: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "#Actual Clustering\n",
    "\n",
    "#Get centroid created in initial step\n",
    "centroids = np.genfromtxt(os.path.join(base_path,\"centroids.csv\"), delimiter=',')\n",
    "features = np.genfromtxt(os.path.join(base_path,\"features.csv\"), delimiter=',')\n",
    "sample_count = 1\n",
    "for filename in os.listdir(sample_path):\n",
    "    tdf = pd.read_csv(sample_path+filename, index_col=0)\n",
    "    #Filter Columns\n",
    "    t = tdf[['ip.dst', 'ip.proto', 'sniff_timestamp', 'sample']]\n",
    "    #Remove null destinations\n",
    "    t = t[t['ip.dst'].notnull()]\n",
    "    #Rename Columns\n",
    "    t.columns = ['ip', 'proto', 'time_stamp', 'sample']\n",
    "    #Get count for each ip\n",
    "    df = t.groupby(['ip', 'proto']).size().unstack().fillna(0).astype(int)\n",
    "    #Select TCP and UDP as only fetures (TCP:6, UDP:17)\n",
    "    df = df[[6,17]]\n",
    "    if(set(df.columns) != set(features)):\n",
    "        print(df.columns, features)\n",
    "        non_columns = set(features) - set(df.columns)\n",
    "        for c in non_columns:\n",
    "            df.insert(loc=1, column=c, value=0)\n",
    "    #Get value matrix\n",
    "    X = df.values\n",
    "    #Create scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #Transform Traning data\n",
    "    X_trans = scaler.transform(X)\n",
    "    #Data Fitting using K-means\n",
    "    kmeans = KMeans(n_clusters=centroids.shape[0], init=centroids)\n",
    "    clusters = kmeans.fit_predict(X_trans)\n",
    "    #Plot clusters and data using PCA component analysis\n",
    "    #draw_clusters(X_trans, clusters, centroids, kmeans)\n",
    "    distances = k_mean_dist(X_trans, clusters, centroids)\n",
    "    #Attaching label/cluster to IP\n",
    "    cluster_df = pd.DataFrame({'cluster': kmeans.labels_})\n",
    "    #Attaching distance from the cluster for each data point\n",
    "    distance_df = pd.DataFrame({'distance': distances})\n",
    "    ip_label_df = pd.concat([df.reset_index(), cluster_df, distance_df], axis=1).set_index('ip')\n",
    "    if not os.path.exists(cluster_path):\n",
    "        os.makedirs(cluster_path)\n",
    "    ip_label_df.to_csv(cluster_path+str(sample_count))\n",
    "    sample_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For a given IP address find how many time a given cluster it was assigned to.\n",
    "from itertools import groupby\n",
    "def get_IP_cluster_count_dict(cluster_path):    \n",
    "    ip_dict = dict()\n",
    "    filenames = glob.glob(os.path.join(cluster_path,'*'))\n",
    "    for filename in filenames:\n",
    "        with open(filename, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if row['ip'] in ip_dict:\n",
    "                    #print(row['ip'],ip_dict[row['ip']])\n",
    "                    ip_dict[row['ip']] = ip_dict[row['ip']] + [row['cluster']]\n",
    "                else:\n",
    "                    ip_dict[row['ip']] = [row['cluster']]\n",
    "    #Find how many time IP was assigned to a given cluster\n",
    "    ip_cluster_dict = dict()\n",
    "    for key, value in ip_dict.items():\n",
    "        ip_cluster_dict[key] = {k: len(list(group)) for k, group in groupby(value)}\n",
    "    return ip_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_cluster_dict = get_IP_cluster_count_dict(cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ip_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([np.arange(5)]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals = [-114.0, -9.3187241903579459, -4.3769387148407386, -1.8276073696951562,\n",
    "        -1.111777920816883, -0.73454965159430574, -0.50130043613458697, -0.32255237503735512, -0.23209280297904877]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_diff = abs(vals[0] - vals[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006292015617713829\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "elbow_val = []\n",
    "for i in range(8):\n",
    "    diff = abs(vals[i] - vals[i+1])/ total_diff\n",
    "    if diff < 0.01:\n",
    "        elbow_val.append(i)\n",
    "        print(diff)\n",
    "        break\n",
    "\n",
    "print(elbow_val)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elbow_val = [3,4,3,3,3,4,4,4,3,3,3,3,3,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(np.mean(elbow_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
