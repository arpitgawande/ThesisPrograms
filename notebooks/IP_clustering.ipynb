{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Common Imports\n",
    "import numpy as np\n",
    "#Pandas for creating dataframes\n",
    "import pandas as pd\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "#K-means clustering algo\n",
    "from sklearn.cluster import KMeans\n",
    "#OS moduled for file oprations\n",
    "import os\n",
    "#CSV module\n",
    "import csv\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Base Folder Paths\n",
    "base_folder = 'converted'\n",
    "test_folder = 'test2'\n",
    "base_path = os.path.join(base_folder, test_folder)\n",
    "#Normal Sample and cluster paths\n",
    "sample_dir_name = 'samp'\n",
    "sample_path = os.path.join(base_path, sample_dir_name)\n",
    "#If sample folder exist\n",
    "if os.path.isdir(sample_path):\n",
    "    cluster_dir_name = sample_dir_name+'_cluster'\n",
    "    cluster_path = os.path.join(base_path, cluster_dir_name)\n",
    "    os.makedirs(cluster_path, exist_ok=True)\n",
    "    #Attack Sample and Cluster paths\n",
    "    # sample_path = os.path.join(base_path,'attack_samples','1')\n",
    "    # cluster_path = os.path.join(base_path,'attack_ip_cluster','1')\n",
    "    os.makedirs(cluster_path, exist_ok=True)\n",
    "    #File to store centroids\n",
    "    centroid_filename = 'centroids.csv'\n",
    "    #File to store feature list\n",
    "    features_filename = 'features.csv'\n",
    "#Defining features\n",
    "features = [6,17] #(TCP:6, UDP:17)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "#Merge sample files to create bigger sameple\n",
    "def merge_sample_files(sample_path, merge_count):\n",
    "    #Make copy\n",
    "    head, tail = os.path.split(sample_path)\n",
    "    copy_path = os.path.join(head,tail+'_copy')\n",
    "    if os.path.isdir(copy_path):\n",
    "        shutil.rmtree(copy_path)\n",
    "    shutil.copytree(sample_path, copy_path)\n",
    "    #delete all the file and recreate folder\n",
    "    shutil.rmtree(sample_path)\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    file_number = 1\n",
    "    count = 0\n",
    "    filenames = sorted(glob.glob(os.path.join(copy_path,'*')),  key=os.path.getmtime)\n",
    "    for filename in filenames:\n",
    "        if count == 0:\n",
    "            df = pd.read_csv(filename, index_col=0)\n",
    "            count += 1\n",
    "        else:\n",
    "            temp_df = pd.read_csv(filename, index_col=0)\n",
    "            df = df.append(temp_df)\n",
    "            count += 1\n",
    "        if count == merge_count:\n",
    "            df.to_csv(os.path.join(sample_path,str(file_number)))\n",
    "            df = df.drop(df.index, inplace=True)\n",
    "            count = 0\n",
    "            file_number +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge_count = 3\n",
    "# merge_sample_files(sample_path, merge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_dataframe(sample_file, features):\n",
    "        df = pd.read_csv(sample_file, index_col=0)\n",
    "        #Filter Columns\n",
    "        df = df[['ip.dst', 'ip.proto', 'sniff_timestamp', 'sample']]\n",
    "        #Remove null destinations\n",
    "        df = df[df['ip.dst'].notnull()]\n",
    "        #Rename Columns\n",
    "        df.columns = ['ip', 'protocol', 'time_stamp', 'sample']\n",
    "        #Get count for each ip\n",
    "        df = df.groupby(['ip', 'protocol']).size().unstack().fillna(0).astype(int)\n",
    "        #Select TCP and UDP as only fetures (TCP:6, UDP:17)\n",
    "        df = df[features]\n",
    "        if(set(df.columns) != set(features)):\n",
    "            print(df.columns, features)\n",
    "            non_columns = set(features) - set(df.columns)\n",
    "            for c in non_columns:\n",
    "                df.insert(loc=features.index(c), column=c, value=0)\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = sorted(glob.glob(os.path.join(sample_path,'*')),  key=os.path.getmtime)\n",
    "df = get_feature_dataframe(filenames[0], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#Find optimal number of clusters for k-means clustering using elbow method.\n",
    "def elbow_method(X_trans):\n",
    "    elbow_count = 0\n",
    "    range_val = 10\n",
    "    nc = range(1, range_val)\n",
    "    kmeans = [KMeans(n_clusters=i) for i in nc]\n",
    "    score = [kmeans[i].fit(X_trans).score(X_trans) for i in range(len(kmeans))]\n",
    "    total_diff = abs(score[0] - score[len(score) -1])\n",
    "    for i in range(range_val - 2):\n",
    "        percent_diff = abs(score[i] - score[i+1])/total_diff\n",
    "        if percent_diff < 0.01:\n",
    "            opt_clust_count = i\n",
    "            break\n",
    "#     plt.plot(nc,score)\n",
    "#     plt.xlabel('Number of Clusters')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title('Elbow Curve')\n",
    "#     plt.show()\n",
    "    return opt_clust_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
    "def PCA_component_analysis(X_std):\n",
    "    mean_vec = np.mean(X_std, axis=0)\n",
    "    cov_mat = np.cov(X_std.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "    # Create a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "    # # Sort from high to low\n",
    "    eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "    # Calculation of Explained Variance from the eigenvalues\n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "    cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "    # PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(9), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
    "    plt.step(range(9), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature dataframes from the sample files\n",
    "def get_all_dataframes(sample_path, features):\n",
    "    sample_df_list = []\n",
    "    for filename in os.listdir(sample_path):\n",
    "        sample_file = os.path.join(sample_path,filename)\n",
    "        df = get_feature_dataframe(sample_file, features)\n",
    "        sample_df_list.append(df)\n",
    "    return sample_df_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kmeans_centroid(feature_df, cluster_count):\n",
    "    \"\"\" X -> feature vector \n",
    "        cluster_count -> Number of clusters to be used for k-means\n",
    "    \"\"\"\n",
    "    df_centroid = {}\n",
    "    X = feature_df.values\n",
    "    #Create scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #Transform Traning data\n",
    "    X_trans = scaler.transform(X)\n",
    "    #Data Fitting using K-means\n",
    "    kmeans = KMeans(n_clusters=cluster_count)\n",
    "    kmeans.fit(X_trans)\n",
    "    #Insert cluster center to its corrosposnding dataframe each dataframe.\n",
    "    #Dataframe 0 contain all the clusters centers associated with 0th cluster\n",
    "    first = True\n",
    "    for i in range(kmeans.cluster_centers_.shape[0]):\n",
    "        s = pd.Series(kmeans.cluster_centers_[i], index=feature_df.columns)\n",
    "        if(first):\n",
    "            df_centroid = pd.DataFrame(columns=feature_df.columns)\n",
    "            first = False\n",
    "        df_centroid = df_centroid.append(s,ignore_index=True)\n",
    "    return df_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first = True\n",
    "ip_dict = dict()\n",
    "sample_count = 1;\n",
    "centroid_dfs = []\n",
    "first = True\n",
    "cluster_count = 3\n",
    "features = [6,17] #(TCP:6, UDP:17)\n",
    "\n",
    "\n",
    "sample_df_list = get_all_dataframes(sample_path, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_store_centroid_median(sample_frames, centroid_filename, features_filename):\n",
    "    features = sample_frames[0].columns\n",
    "    df_concat = pd.DataFrame(columns=features)\n",
    "    for df in sample_frames:\n",
    "        df_centroid = get_kmeans_centroid(df, cluster_count)\n",
    "        df_concat = df_concat.append(df_centroid)\n",
    "    #This gives list of centroid names\n",
    "    clusters = df_concat.index.unique()\n",
    "    centroids = []\n",
    "    #Find median for each centroid and store them in file\n",
    "    for c in clusters:\n",
    "        med = np.median(df_concat.loc[c], axis=0)\n",
    "        centroids.append(med) \n",
    "    np.savetxt(os.path.join(base_path, centroid_filename), np.asarray(centroids), delimiter=\",\")\n",
    "    np.savetxt(os.path.join(base_path, features_filename), np.asarray(list(features)), delimiter=\",\")\n",
    "    return np.asarray(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_centroid_features(centroid_filename, features_filename):\n",
    "    centroids = np.genfromtxt(os.path.join(base_path,centroid_filename), delimiter=',')\n",
    "    features = np.genfromtxt(os.path.join(base_path,features_filename), delimiter=',')\n",
    "    return centroids, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def draw_clusters(X, centroids):\n",
    "    #Use PCA component analysis for visuals\n",
    "    if X.shape[1] > 2:\n",
    "        reduced_X = PCA(n_components=2).fit_transform(X)\n",
    "    else:\n",
    "        reduced_X = X\n",
    "   \n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .01     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_X[:, 0].min() - 1, reduced_X[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_X[:, 1].min() - 1, reduced_X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    kmeans = KMeans(init=centroids)\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')   \n",
    "    #Plot the data points (PCA reduced components)\n",
    "    plt.plot(reduced_X[:,0],reduced_X[:,1],  'k.', markersize=3) \n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=169, linewidths=3, color='w', zorder=10)\n",
    "    plt.title('K-means clustering with (PCA-reduced data), Centroids are marked with white cross')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids, features = read_centroid_features(centroid_filename, features_filename)\n",
    "def kmeans_clustering(feature_df, centroids, file_name):\n",
    "    X = feature_df.values\n",
    "    #Create scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #Transform Traning data\n",
    "    X_trans = scaler.transform(X)\n",
    "    #k means clustering using provided centroids \n",
    "    kmeans = KMeans(init=centroids)\n",
    "    clusters = kmeans.fit_predict(X_trans)\n",
    "    #Plot clusters and data using PCA component analysis\n",
    "    #draw_clusters(X_trans, clusters, centroids, kmeans)\n",
    "    #Find distance of each point from the centroid\n",
    "    distances = k_mean_dist(X_trans, clusters, centroids)\n",
    "    #Getting the labels/clusters and distances of each IP from centroid\n",
    "    cluster_df = pd.DataFrame({'cluster': kmeans.labels_})\n",
    "    distance_df = pd.DataFrame({'distance': distances})\n",
    "    #Attaching label and distance to existing df and write new dataframe to file\n",
    "    df = pd.concat([feature_df.reset_index(), cluster_df, distance_df], axis=1).set_index('ip')\n",
    "    df.to_csv(os.path.join(cluster_path,str(file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "#Calculate distance of feature vector point from the its cluster center\n",
    "def k_mean_dist(feature_vector, label, cluster_centers):\n",
    "    distances = []\n",
    "    for i, d in enumerate(feature_vector):\n",
    "        center = cluster_centers[label[i]]\n",
    "        distance = euclidean(d,center)\n",
    "        #distance = np.linalg.norm(d - center)\n",
    "        distances.append(distance)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For a given IP address find how many time a given cluster it was assigned to.\n",
    "from itertools import groupby\n",
    "def get_IP_cluster_count_dict(cluster_path):    \n",
    "    ip_dict = dict()\n",
    "    filenames = glob.glob(os.path.join(cluster_path,'*'))\n",
    "    for filename in filenames:\n",
    "        with open(filename, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if row['ip'] in ip_dict:\n",
    "                    #print(row['ip'],ip_dict[row['ip']])\n",
    "                    ip_dict[row['ip']] = ip_dict[row['ip']] + [row['cluster']]\n",
    "                else:\n",
    "                    ip_dict[row['ip']] = [row['cluster']]\n",
    "    #Find how many time IP was assigned to a given cluster\n",
    "    ip_cluster_dict = dict()\n",
    "    for key, value in ip_dict.items():\n",
    "        ip_cluster_dict[key] = {k: len(list(group)) for k, group in groupby(value)}\n",
    "    return ip_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_cluster_dict = get_IP_cluster_count_dict(cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ip_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_feature_vector_dict(cluster_path):\n",
    "    cluster_dict = dict()\n",
    "    filenames = sorted(glob.glob(os.path.join(cluster_path,'*')),  key=os.path.getmtime)\n",
    "    first = True\n",
    "    for filename in filenames:\n",
    "        if first:\n",
    "            df = pd.read_csv(filename, index_col=0)\n",
    "            first = False\n",
    "        else:\n",
    "            temp_df = pd.read_csv(filename, index_col=0)\n",
    "            df = df.append(temp_df) \n",
    "    df = df.reset_index().set_index(['cluster','ip'])\n",
    "    clusters = df.index.get_level_values(0).unique()\n",
    "    for c in clusters:\n",
    "        cluster_dict[c] = df.loc[c].iloc[:,:-1].values\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_outlier_detecton(X_train, clf):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "    \n",
    "    # plot the levels lines and the points\n",
    "    print(clf.name)\n",
    "    if clf.name == \"lof\":\n",
    "        # decision_function is private for LOF\n",
    "        Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    print(Z.max(), Z.min())\n",
    "    plt.title(\"Novelty Detection\")\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "    a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "    #plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "    \n",
    "    s = 40\n",
    "    b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\n",
    "    plt.axis('tight')\n",
    "    plt.xlim((-5, 5))\n",
    "    plt.ylim((-5, 5))\n",
    "    plt.legend([a.collections[0], b1],\n",
    "           [\"learned frontier\", \"training observations\"],\n",
    "           loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SKlearn SVM\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "def one_class_svm_for_clusters(cluster_feature_dict):\n",
    "    svm_dict = dict()\n",
    "    scalar_dict = dict()\n",
    "    for key, value in cluster_feature_dict.items():\n",
    "        X_train = value\n",
    "        #Get scaler\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        scalar_dict[key] = scaler \n",
    "        #Transform Traning data\n",
    "        X_trans = scaler.transform(X_train)\n",
    "        # fit the model\n",
    "        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "        clf.fit(X_trans)\n",
    "        clf.name = 'svm'\n",
    "        plot_outlier_detecton(X_trans, clf)\n",
    "        \n",
    "        clf = LocalOutlierFactor(contamination=0.01)\n",
    "        clf.fit(X_trans)\n",
    "        clf.name= 'lof'\n",
    "        plot_outlier_detecton(X_trans, clf)\n",
    "        #Store trained SVM for each IP\n",
    "        svm_dict[key] = clf\n",
    "    return svm_dict, scalar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge_sample_files(sample_path,3)\n",
    "d = get_cluster_feature_vector_dict(cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svm_dict, scaler_dict = one_class_svm_for_clusters(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Predict for the give destination if it is normal or not\n",
    "X_test = [[0,120]]\n",
    "X_test_tran = scaler_dict[0].transform(X_test)\n",
    "svm_dict[0].predict(X_test_tran)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
