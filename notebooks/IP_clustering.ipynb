{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Common Imports\n",
    "import numpy as np\n",
    "#Pandas for creating dataframes\n",
    "import pandas as pd\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "#K-means clustering algo\n",
    "from sklearn.cluster import KMeans\n",
    "#OS moduled for file oprations\n",
    "import os\n",
    "#CSV module\n",
    "import csv\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
    "def PCA_component_analysis(X_std):\n",
    "    mean_vec = np.mean(X_std, axis=0)\n",
    "    cov_mat = np.cov(X_std.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "    # Create a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "    # # Sort from high to low\n",
    "    eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "    # Calculation of Explained Variance from the eigenvalues\n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "    cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "    # PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(9), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
    "    plt.step(range(9), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def plot_clusters(X, X_clusters, centroids, kmeans):\n",
    "    #Use PCA component analysis for visuals\n",
    "    if X.shape[1] > 2:\n",
    "        reduced_X = PCA(n_components=2).fit_transform(X)\n",
    "    else:\n",
    "        reduced_X = X\n",
    "   \n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .01     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_X[:, 0].min() - 1, reduced_X[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_X[:, 1].min() - 1, reduced_X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')   \n",
    "    #Plot the data points (PCA reduced components)\n",
    "    plt.plot(reduced_X[:,0],reduced_X[:,1],  'k.', markersize=3t) \n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=169, linewidths=3, color='w', zorder=10)\n",
    "    plt.title('K-means clustering with (PCA-reduced data), Centroids are marked with white cross')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "def k_mean_dist(data, clusters, cluster_centers):\n",
    "    distances = []\n",
    "    for i, d in enumerate(data):\n",
    "        center = cluster_centers[clusters[i]]\n",
    "        distance = euclidean(d,center)\n",
    "        #distance = np.linalg.norm(d - center)\n",
    "        distances.append(distance)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def determine_cluster_count(X_trans):\n",
    "    Nc = range(1, 10)\n",
    "    kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "    kmeans\n",
    "    score = [kmeans[i].fit(X_trans).score(X_trans) for i in range(len(kmeans))]\n",
    "    score\n",
    "    plt.plot(Nc,score)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Elbow Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "#Merge sample files to create bigger sameple\n",
    "def merge_sample_files(sample_folder, file_count):\n",
    "    file_number = 1\n",
    "    count = 0\n",
    "    filenames = sorted(glob.glob(os.path.join(sample_folder,'*')),  key=os.path.getmtime)\n",
    "    for filename in filenames:\n",
    "        if count == 0:\n",
    "            df = pd.read_csv(filename, index_col=0)\n",
    "            count += 1\n",
    "        else:\n",
    "            temp_df = pd.read_csv(filename, index_col=0)\n",
    "            df = df.append(temp_df)\n",
    "            count += 1\n",
    "        if count == file_count:\n",
    "            df.to_csv(os.path.join(sample_folder,'m'+str(file_number)))\n",
    "            df = df.drop(df.index, inplace=True)\n",
    "            count = 0\n",
    "            file_number +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_feature_vector_dict(cluster_folder):\n",
    "    cluster_dict = dict()\n",
    "    filenames = sorted(glob.glob(os.path.join(cluster_folder,'*')),  key=os.path.getmtime)\n",
    "    first = True\n",
    "    for filename in filenames:\n",
    "        if first:\n",
    "            df = pd.read_csv(filename, index_col=0)\n",
    "            first = False\n",
    "        else:\n",
    "            temp_df = pd.read_csv(filename, index_col=0)\n",
    "            df = df.append(temp_df) \n",
    "    df = df.reset_index().set_index(['cluster','ip'])\n",
    "    cluster = df.index.get_level_values(0).unique()\n",
    "    for c in clusters:\n",
    "        cluster_dict[c] = df.loc[c].iloc[:,:-1].values\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SKlearn SVM\n",
    "from sklearn import svm\n",
    "def one_class_svm_for_clusters(cluster_feature_dict):\n",
    "    svm_dict = dict()\n",
    "    for key, value in cluster_feature_dict.items():\n",
    "        X_train = value\n",
    "        # fit the model\n",
    "        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "        clf.fit(X_train)\n",
    "        #Store trained SVM for each IP\n",
    "        svm_dict[key] = clf\n",
    "    return svm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Base Folder sPaths\n",
    "base_path = os.path.join('converted','test2')\n",
    "#Normal\n",
    "sample_path = os.path.join(base_path,'samples')\n",
    "cluster_path = os.path.join(base_path,'ip_cluster')\n",
    "#Attack\n",
    "# sample_path = os.path.join(base_path,'attack_samples','1')\n",
    "# cluster_path = os.path.join(base_path,'attack_ip_cluster','1')\n",
    "\n",
    "centroid_path = os.path.join(base_path,'centroids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_sample_files(sample_path,3)\n",
    "d = get_cluster_feature_vector_dict(cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_dict = one_class_svm_for_clusters(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict for the give destination if it is normal or not\n",
    "svm_dict['192.168.0.7'].predict([[0,0,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuli\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "ip_dict = dict()\n",
    "sample_count = 1;\n",
    "columns = ['0','1','2','3']\n",
    "centroid_dfs = []\n",
    "first = True\n",
    "\n",
    "for filename in os.listdir(sample_path):\n",
    "    tdf = pd.read_csv(sample_path+filename, index_col=0)\n",
    "    #Filter Columns\n",
    "    t = tdf[['ip.dst', 'ip.proto', 'sniff_timestamp', 'sample']]\n",
    "    #Remove null destinations\n",
    "    t = t[t['ip.dst'].notnull()]\n",
    "    #Rename Columns\n",
    "    t.columns = ['ip', 'proto', 'time_stamp', 'sample']\n",
    "    #Get count for each ip\n",
    "    df = t.groupby(['ip', 'proto']).size().unstack().fillna(0).astype(int)\n",
    "    #Select TCP and UDP as only fetures (TCP:6, UDP:17)\n",
    "    df = df[[6,17]]\n",
    "    #Get value matrix\n",
    "    X = df.values\n",
    "    #Create scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #Transform Traning data\n",
    "    X_trans = scaler.transform(X)\n",
    "    #print(X_trans)\n",
    "    #Determine cluster counts using elbow method\n",
    "    #determine_cluster_count(X_trans)\n",
    "    #Define Number of Clusters\n",
    "    cluster_count = 3\n",
    "    #Data Fitting using K-means\n",
    "    #if first:\n",
    "    kmeans = KMeans(n_clusters=cluster_count)\n",
    "    kmeans.fit(X_trans)\n",
    "    #Insert cluster center to its corrosposnding dataframe each dataframe.\n",
    "    #Dataframe 0 contain all the clusters centers associated with 0th cluster \n",
    "    for i in range(kmeans.cluster_centers_.shape[0]):\n",
    "        s = pd.Series(kmeans.cluster_centers_[i], index=df.columns)\n",
    "        if(first):\n",
    "            centroid_dfs.append(pd.DataFrame(columns=df.columns))\n",
    "        centroid_dfs[i] = centroid_dfs[i].append(s,ignore_index=True)         \n",
    "    first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Centroid Mean\n",
    "centroids = []\n",
    "features = set()\n",
    "for df in centroid_dfs:\n",
    "    centroid = []\n",
    "    for c in df.columns:\n",
    "        df = df[np.abs(df[c] - df[c].mean()) <= (3*df[c].std())]\n",
    "        #print(df[c])\n",
    "        centroid.append(df[c].mean())\n",
    "    centroids.append(centroid)\n",
    "    features |= set(df.columns)\n",
    "#Save centroid for future clusterinng\n",
    "if not os.path.exists(centroid_path):\n",
    "    os.makedirs(centroid_path)\n",
    "np.savetxt(centroid_path+\"centroids.csv\", np.asarray(centroids), delimiter=\",\")\n",
    "np.savetxt(centroid_path+\"features.csv\", np.asarray(list(features)), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuli\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Tuli\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:896: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "#Actual Clustering\n",
    "\n",
    "#Get centroid created in initial step\n",
    "centroids = np.genfromtxt(centroid_path+\"centroids.csv\", delimiter=',')\n",
    "features = np.genfromtxt(centroid_path+\"features.csv\", delimiter=',')\n",
    "sample_count = 1\n",
    "for filename in os.listdir(sample_path):\n",
    "    tdf = pd.read_csv(sample_path+filename, index_col=0)\n",
    "    #Filter Columns\n",
    "    t = tdf[['ip.dst', 'ip.proto', 'sniff_timestamp', 'sample']]\n",
    "    #Remove null destinations\n",
    "    t = t[t['ip.dst'].notnull()]\n",
    "    #Rename Columns\n",
    "    t.columns = ['ip', 'proto', 'time_stamp', 'sample']\n",
    "    #Get count for each ip\n",
    "    df = t.groupby(['ip', 'proto']).size().unstack().fillna(0).astype(int)\n",
    "    #Select TCP and UDP as only fetures (TCP:6, UDP:17)\n",
    "    df = df[[6,17]]\n",
    "    if(set(df.columns) != set(features)):\n",
    "        print(df.columns, features)\n",
    "        non_columns = set(features) - set(df.columns)\n",
    "        for c in non_columns:\n",
    "            df.insert(loc=1, column=c, value=0)\n",
    "    #Get value matrix\n",
    "    X = df.values\n",
    "    #Create scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #Transform Traning data\n",
    "    X_trans = scaler.transform(X)\n",
    "    #Data Fitting using K-means\n",
    "    kmeans = KMeans(n_clusters=centroids.shape[0], init=centroids)\n",
    "    clusters = kmeans.fit_predict(X_trans)\n",
    "    #Plot clusters and data using PCA component analysis\n",
    "    #plot_clusters(X_trans, clusters, centroids, kmeans)\n",
    "    distances = k_mean_dist(X_trans, clusters, centroids)\n",
    "    #Attaching label/cluster to IP\n",
    "    cluster_df = pd.DataFrame({'cluster': kmeans.labels_})\n",
    "    #Attaching distance from the cluster for each data point\n",
    "    distance_df = pd.DataFrame({'distance': distances})\n",
    "    ip_label_df = pd.concat([df.reset_index(), cluster_df, distance_df], axis=1).set_index('ip')\n",
    "    if not os.path.exists(cluster_path):\n",
    "        os.makedirs(cluster_path)\n",
    "    ip_label_df.to_csv(cluster_path+str(sample_count))\n",
    "    sample_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "ip_dict = dict()\n",
    "\n",
    "for filename in os.listdir(cluster_path):\n",
    "    with open(cluster_path+filename, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['ip'] in ip_dict:\n",
    "                #print(row['ip'],ip_dict[row['ip']])\n",
    "                ip_dict[row['ip']] = ip_dict[row['ip']] + [row['cluster']]\n",
    "            else:\n",
    "                ip_dict[row['ip']] = [row['cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_cluster_dict = dict()\n",
    "#Find count of clusters \n",
    "for key, value in ip_dict.items():\n",
    "    ip_cluster_dict[key] = {k: len(list(group)) for k, group in groupby(value)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'104.208.156.39': {'0': 1},\n",
       " '172.217.10.109': {'0': 1},\n",
       " '172.217.10.110': {'0': 1},\n",
       " '172.217.10.14': {'0': 1},\n",
       " '172.217.10.227': {'0': 1},\n",
       " '172.217.10.234': {'0': 1},\n",
       " '172.217.10.238': {'0': 1},\n",
       " '172.217.12.174': {'0': 1},\n",
       " '172.217.9.238': {'0': 1},\n",
       " '173.194.204.189': {'0': 1},\n",
       " '173.194.206.189': {'0': 1},\n",
       " '173.194.208.189': {'0': 1},\n",
       " '173.194.68.189': {'0': 1},\n",
       " '192.168.0.10': {'2': 1},\n",
       " '192.168.0.4': {'0': 1},\n",
       " '192.168.0.7': {'1': 1},\n",
       " '216.58.219.227': {'0': 1},\n",
       " '216.58.219.229': {'0': 1},\n",
       " '224.0.0.251': {'0': 1},\n",
       " '52.1.34.253': {'2': 1},\n",
       " '52.200.131.105': {'0': 1},\n",
       " '54.175.13.65': {'0': 1},\n",
       " '54.84.217.201': {'0': 1},\n",
       " '65.19.96.252': {'0': 1},\n",
       " '65.19.96.253': {'0': 1},\n",
       " '65.52.108.76': {'0': 1}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Group IP by clusters and write to file\n",
    "for filename in os.listdir(cluster_path):\n",
    "    df = pd.read_csv(cluster_path+filename, index_col=0)\n",
    "    \n",
    "    if not os.path.exists(base_directory):\n",
    "                    os.makedirs(base_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([np.arange(5)]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[0] = df[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0  1  1  2  3  4\n",
       "1  1  1  2  3  4\n",
       "2  1  1  2  3  4"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.arange(10)]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
