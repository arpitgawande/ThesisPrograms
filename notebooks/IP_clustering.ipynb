{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Common Imports\n",
    "import numpy as np\n",
    "#Pandas for creating dataframes\n",
    "import pandas as pd\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "#K-means clustering algo\n",
    "from sklearn.cluster import KMeans\n",
    "#OS moduled for file oprations\n",
    "import os\n",
    "#CSV module\n",
    "import csv\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
    "def PCA_component_analysis(X_std):\n",
    "    mean_vec = np.mean(X_std, axis=0)\n",
    "    cov_mat = np.cov(X_std.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "    # Create a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "    # # Sort from high to low\n",
    "    eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "    # Calculation of Explained Variance from the eigenvalues\n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "    cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "    # PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(9), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
    "    plt.step(range(9), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def draw_clusters(X, X_clusters, centroids, kmeans):\n",
    "    #Use PCA component analysis for visuals\n",
    "    if X.shape[1] > 2:\n",
    "        reduced_X = PCA(n_components=2).fit_transform(X)\n",
    "    else:\n",
    "        reduced_X = X\n",
    "   \n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .01     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_X[:, 0].min() - 1, reduced_X[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_X[:, 1].min() - 1, reduced_X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')   \n",
    "    #Plot the data points (PCA reduced components)\n",
    "    plt.plot(reduced_X[:,0],reduced_X[:,1],  'k.', markersize=3) \n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=169, linewidths=3, color='w', zorder=10)\n",
    "    plt.title('K-means clustering with (PCA-reduced data), Centroids are marked with white cross')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "#Calculate distance of data point from the its cluster center\n",
    "def k_mean_dist(data, clusters, cluster_centers):\n",
    "    distances = []\n",
    "    for i, d in enumerate(data):\n",
    "        center = cluster_centers[clusters[i]]\n",
    "        distance = euclidean(d,center)\n",
    "        #distance = np.linalg.norm(d - center)\n",
    "        distances.append(distance)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#Find optimal number of clusters for k-means clustering using elbow method.\n",
    "def elbow_method(X_trans):\n",
    "    elbow_count = 0\n",
    "    range_val = 10\n",
    "    Nc = range(1, range_val)\n",
    "    kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "    score = [kmeans[i].fit(X_trans).score(X_trans) for i in range(len(kmeans))]\n",
    "    total_diff = abs(score[0] - score[len(score) -1])\n",
    "    for i in range(range_val - 2):\n",
    "        percent_diff = abs(score[i] - score[i+1])/total_diff\n",
    "        if percent_diff < 0.01:\n",
    "            elbow_count = i\n",
    "            break\n",
    "    plt.plot(Nc,score)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Elbow Curve')\n",
    "    plt.show()\n",
    "    return elbow_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "#Merge sample files to create bigger sameple\n",
    "def merge_sample_files(sample_folder, file_count):\n",
    "    file_number = 1\n",
    "    count = 0\n",
    "    filenames = sorted(glob.glob(os.path.join(sample_folder,'*')),  key=os.path.getmtime)\n",
    "    for filename in filenames:\n",
    "        if count == 0:\n",
    "            df = pd.read_csv(filename, index_col=0)\n",
    "            count += 1\n",
    "        else:\n",
    "            temp_df = pd.read_csv(filename, index_col=0)\n",
    "            df = df.append(temp_df)\n",
    "            count += 1\n",
    "        if count == file_count:\n",
    "            df.to_csv(os.path.join(sample_folder,'m'+str(file_number)))\n",
    "            df = df.drop(df.index, inplace=True)\n",
    "            count = 0\n",
    "            file_number +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_feature_vector_dict(cluster_folder):\n",
    "    cluster_dict = dict()\n",
    "    filenames = sorted(glob.glob(os.path.join(cluster_folder,'*')),  key=os.path.getmtime)\n",
    "    first = True\n",
    "    for filename in filenames:\n",
    "        if first:\n",
    "            df = pd.read_csv(filename, index_col=0)\n",
    "            first = False\n",
    "        else:\n",
    "            temp_df = pd.read_csv(filename, index_col=0)\n",
    "            df = df.append(temp_df) \n",
    "    df = df.reset_index().set_index(['cluster','ip'])\n",
    "    clusters = df.index.get_level_values(0).unique()\n",
    "    for c in clusters:\n",
    "        cluster_dict[c] = df.loc[c].iloc[:,:-1].values\n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_outlier_detecton(X_train, clf):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "    \n",
    "    # plot the levels lines and the points\n",
    "    print(clf.name)\n",
    "    if clf.name == \"lof\":\n",
    "        # decision_function is private for LOF\n",
    "        Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    print(Z.max(), Z.min())\n",
    "    plt.title(\"Novelty Detection\")\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "    a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "    #plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "    \n",
    "    s = 40\n",
    "    b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\n",
    "    plt.axis('tight')\n",
    "    plt.xlim((-5, 5))\n",
    "    plt.ylim((-5, 5))\n",
    "    plt.legend([a.collections[0], b1],\n",
    "           [\"learned frontier\", \"training observations\"],\n",
    "           loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SKlearn SVM\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "def one_class_svm_for_clusters(cluster_feature_dict):\n",
    "    svm_dict = dict()\n",
    "    scalar_dict = dict()\n",
    "    for key, value in cluster_feature_dict.items():\n",
    "        X_train = value\n",
    "        #Get scaler\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        scalar_dict[key] = scaler \n",
    "        #Transform Traning data\n",
    "        X_trans = scaler.transform(X_train)\n",
    "        # fit the model\n",
    "        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "        clf.fit(X_trans)\n",
    "        clf.name = 'svm'\n",
    "        plot_outlier_detecton(X_trans, clf)\n",
    "        \n",
    "        clf = LocalOutlierFactor(contamination=0.01)\n",
    "        clf.fit(X_trans)\n",
    "        clf.name= 'lof'\n",
    "        plot_outlier_detecton(X_trans, clf)\n",
    "        #Store trained SVM for each IP\n",
    "        svm_dict[key] = clf\n",
    "    return svm_dict, scalar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Base Folder sPaths\n",
    "base_path = os.path.join('converted','test2')\n",
    "#Normal\n",
    "sample_path = os.path.join(base_path,'samples')\n",
    "cluster_path = os.path.join(base_path,'ip_cluster')\n",
    "#Attack\n",
    "# sample_path = os.path.join(base_path,'attack_samples','1')\n",
    "# cluster_path = os.path.join(base_path,'attack_ip_cluster','1')\n",
    "\n",
    "centroid_path = os.path.join(base_path,'centroids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge_sample_files(sample_path,3)\n",
    "d = get_cluster_feature_vector_dict(cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svm_dict, scaler_dict = one_class_svm_for_clusters(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
       "       max_iter=-1, nu=0.1, random_state=None, shrinking=True, tol=0.001,\n",
       "       verbose=False),\n",
       " 1: OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
       "       max_iter=-1, nu=0.1, random_state=None, shrinking=True, tol=0.001,\n",
       "       verbose=False),\n",
       " 2: OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
       "       max_iter=-1, nu=0.1, random_state=None, shrinking=True, tol=0.001,\n",
       "       verbose=False)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict for the give destination if it is normal or not\n",
    "X_test = [[0,120]]\n",
    "X_test_tran = scaler_dict[0].transform(X_test)\n",
    "svm_dict[0].predict(X_test_tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_dataframe(sample_file, features):\n",
    "        df = pd.read_csv(sample_file, index_col=0)\n",
    "        #Filter Columns\n",
    "        df = df[['ip.dst', 'ip.proto', 'sniff_timestamp', 'sample']]\n",
    "        #Remove null destinations\n",
    "        df = df[df['ip.dst'].notnull()]\n",
    "        #Rename Columns\n",
    "        df.columns = ['ip', 'protocol', 'time_stamp', 'sample']\n",
    "        #Get count for each ip\n",
    "        df = df.groupby(['ip', 'protocol']).size().unstack().fillna(0).astype(int)\n",
    "        #Select TCP and UDP as only fetures (TCP:6, UDP:17)\n",
    "        df = df[features]\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature dataframes from the sample files\n",
    "def get_all_dataframes(sample_path, features):\n",
    "    sample_df_list = []\n",
    "    for filename in os.listdir(sample_path):\n",
    "        sample_file = os.path.join(sample_path,filename)\n",
    "        df = create_feature_dataframe(sample_file, features)\n",
    "        sample_df_list.append(df)\n",
    "    return sample_df_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kmeans_centroid(feature_df, cluster_count):\n",
    "    \"\"\" X -> feature vector \n",
    "        cluster_count -> Number of clusters to be used for k-means\n",
    "    \"\"\"\n",
    "    df_centroid = {}\n",
    "    X = feature_df.values\n",
    "    #Create scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #Transform Traning data\n",
    "    X_trans = scaler.transform(X)\n",
    "    #Data Fitting using K-means\n",
    "    kmeans = KMeans(n_clusters=cluster_count)\n",
    "    kmeans.fit(X_trans)\n",
    "    #Insert cluster center to its corrosposnding dataframe each dataframe.\n",
    "    #Dataframe 0 contain all the clusters centers associated with 0th cluster\n",
    "    first = True\n",
    "    for i in range(kmeans.cluster_centers_.shape[0]):\n",
    "        s = pd.Series(kmeans.cluster_centers_[i], index=feature_df.columns)\n",
    "        if(first):\n",
    "            df_centroid = pd.DataFrame(columns=feature_df.columns)\n",
    "            first = False\n",
    "        df_centroid = df_centroid.append(s,ignore_index=True)\n",
    "    return df_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protocol        6         17\n",
      "0        -0.166345 -0.127190\n",
      "1         7.076639  7.264017\n",
      "2         2.072327 -0.268560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpit/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "first = True\n",
    "ip_dict = dict()\n",
    "sample_count = 1;\n",
    "centroid_dfs = []\n",
    "first = True\n",
    "cluster_count = 3\n",
    "features = [6,17] #(TCP:6, UDP:17)\n",
    "\n",
    "\n",
    "sample_df_list = get_all_dataframes(sample_path, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpit/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "df_concat = pd.DataFrame(columns=sample_df_list[0].columns)\n",
    "for df in sample_df_list:\n",
    "    df_centroid = get_kmeans_centroid(df, cluster_count)\n",
    "    df_concat = df_concat.append(df_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [6, 17]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.15796935, -0.16722516]),\n",
       " array([ 5.70622974,  5.58412517]),\n",
       " array([ 1.43205544, -0.12828593])]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = df_concat.index.unique()\n",
    "centroids = []\n",
    "print(centroid)\n",
    "for c in clusters:\n",
    "    med = np.median(df_concat.loc[c], axis=0)\n",
    "    centroids.append(med)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>protocol</th>\n",
       "      <th>6</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.166345</td>\n",
       "      <td>-0.127190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.076639</td>\n",
       "      <td>7.264017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.072327</td>\n",
       "      <td>-0.268560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.220529</td>\n",
       "      <td>-0.156230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.349025</td>\n",
       "      <td>5.816018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "protocol        6         17\n",
       "0        -0.166345 -0.127190\n",
       "1         7.076639  7.264017\n",
       "2         2.072327 -0.268560\n",
       "0        -0.220529 -0.156230\n",
       "1         5.349025  5.816018"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>protocol</th>\n",
       "      <th>6</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.166345</td>\n",
       "      <td>-0.127190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.220529</td>\n",
       "      <td>-0.156230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.090036</td>\n",
       "      <td>5.875205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.200142</td>\n",
       "      <td>-0.152828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.354126</td>\n",
       "      <td>0.544505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "protocol        6         17\n",
       "0        -0.166345 -0.127190\n",
       "0        -0.220529 -0.156230\n",
       "0         6.090036  5.875205\n",
       "0        -0.200142 -0.152828\n",
       "0        -0.354126  0.544505"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_concat\n",
    "#df = df.sort_index()\n",
    "df = df.loc[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0361174a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = plt.boxplot(df[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15796935, -0.16722516])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(df, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]),\n",
       " array([-0.34233056, -0.34379134, -0.34593079, -0.37158622, -0.34250916,\n",
       "        -0.32285204, -0.34742336, -0.3585919 ,  5.87520501,  0.54450508,\n",
       "         0.32278263,  4.83211222,  5.68956244,  6.23478657,  1.4282741 ,\n",
       "         3.56862271,  0.29162963,  5.88973673,  0.39332082,  5.08139928,\n",
       "         4.34111223,  4.98506756]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[\"fliers\"][0].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mean = df.mean()\n",
    "# print(mean)\n",
    "# std = df.std()\n",
    "# print(3*std)\n",
    "# print(\"****\")\n",
    "# df[np.abs(df - mean) <= 3*std]\n",
    "# for index, row in df.iterrows():\n",
    "#     if (np.abs(row - mean) > 3*std).any():\n",
    "#         print(\"outlier\")\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 15,  1, 10, 18, 20])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "a = [1,1,10,20,15,18]\n",
    "b = stats.trimboth(a, 0.1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate Centroid Mean\n",
    "centroids = []\n",
    "features = set()\n",
    "for df in centroid_dfs:\n",
    "    centroid = []\n",
    "    for c in df.columns:\n",
    "        df = df[np.abs(df[c] - df[c].mean()) <= (3*df[c].std())]\n",
    "        #print(df[c])\n",
    "        centroid.append(df[c].mean())\n",
    "    centroids.append(centroid)\n",
    "    features |= set(df.columns)\n",
    "#Save centroid for future clusterinng\n",
    "if not os.path.exists(centroid_path):\n",
    "    os.makedirs(centroid_path)\n",
    "np.savetxt(centroid_path+\"centroids.csv\", np.asarray(centroids), delimiter=\",\")\n",
    "np.savetxt(centroid_path+\"features.csv\", np.asarray(list(features)), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuli\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Tuli\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:896: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "#Actual Clustering\n",
    "\n",
    "#Get centroid created in initial step\n",
    "centroids = np.genfromtxt(centroid_path+\"centroids.csv\", delimiter=',')\n",
    "features = np.genfromtxt(centroid_path+\"features.csv\", delimiter=',')\n",
    "sample_count = 1\n",
    "for filename in os.listdir(sample_path):\n",
    "    tdf = pd.read_csv(sample_path+filename, index_col=0)\n",
    "    #Filter Columns\n",
    "    t = tdf[['ip.dst', 'ip.proto', 'sniff_timestamp', 'sample']]\n",
    "    #Remove null destinations\n",
    "    t = t[t['ip.dst'].notnull()]\n",
    "    #Rename Columns\n",
    "    t.columns = ['ip', 'proto', 'time_stamp', 'sample']\n",
    "    #Get count for each ip\n",
    "    df = t.groupby(['ip', 'proto']).size().unstack().fillna(0).astype(int)\n",
    "    #Select TCP and UDP as only fetures (TCP:6, UDP:17)\n",
    "    df = df[[6,17]]\n",
    "    if(set(df.columns) != set(features)):\n",
    "        print(df.columns, features)\n",
    "        non_columns = set(features) - set(df.columns)\n",
    "        for c in non_columns:\n",
    "            df.insert(loc=1, column=c, value=0)\n",
    "    #Get value matrix\n",
    "    X = df.values\n",
    "    #Create scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    #Transform Traning data\n",
    "    X_trans = scaler.transform(X)\n",
    "    #Data Fitting using K-means\n",
    "    kmeans = KMeans(n_clusters=centroids.shape[0], init=centroids)\n",
    "    clusters = kmeans.fit_predict(X_trans)\n",
    "    #Plot clusters and data using PCA component analysis\n",
    "    #draw_clusters(X_trans, clusters, centroids, kmeans)\n",
    "    distances = k_mean_dist(X_trans, clusters, centroids)\n",
    "    #Attaching label/cluster to IP\n",
    "    cluster_df = pd.DataFrame({'cluster': kmeans.labels_})\n",
    "    #Attaching distance from the cluster for each data point\n",
    "    distance_df = pd.DataFrame({'distance': distances})\n",
    "    ip_label_df = pd.concat([df.reset_index(), cluster_df, distance_df], axis=1).set_index('ip')\n",
    "    if not os.path.exists(cluster_path):\n",
    "        os.makedirs(cluster_path)\n",
    "    ip_label_df.to_csv(cluster_path+str(sample_count))\n",
    "    sample_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For a given IP address find how many time a given cluster it was assigned to.\n",
    "from itertools import groupby\n",
    "def get_IP_cluster_count_dict(cluster_path):    \n",
    "    ip_dict = dict()\n",
    "    filenames = glob.glob(os.path.join(cluster_path,'*'))\n",
    "    for filename in filenames:\n",
    "        with open(filename, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if row['ip'] in ip_dict:\n",
    "                    #print(row['ip'],ip_dict[row['ip']])\n",
    "                    ip_dict[row['ip']] = ip_dict[row['ip']] + [row['cluster']]\n",
    "                else:\n",
    "                    ip_dict[row['ip']] = [row['cluster']]\n",
    "    #Find how many time IP was assigned to a given cluster\n",
    "    ip_cluster_dict = dict()\n",
    "    for key, value in ip_dict.items():\n",
    "        ip_cluster_dict[key] = {k: len(list(group)) for k, group in groupby(value)}\n",
    "    return ip_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip_cluster_dict = get_IP_cluster_count_dict(cluster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ip_cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([np.arange(5)]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals = [-114.0, -9.3187241903579459, -4.3769387148407386, -1.8276073696951562,\n",
    "        -1.111777920816883, -0.73454965159430574, -0.50130043613458697, -0.32255237503735512, -0.23209280297904877]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_diff = abs(vals[0] - vals[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006292015617713829\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "elbow_val = []\n",
    "for i in range(8):\n",
    "    diff = abs(vals[i] - vals[i+1])/ total_diff\n",
    "    if diff < 0.01:\n",
    "        elbow_val.append(i)\n",
    "        print(diff)\n",
    "        break\n",
    "\n",
    "print(elbow_val)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elbow_val = [3,4,3,3,3,4,4,4,3,3,3,3,3,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(np.mean(elbow_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
